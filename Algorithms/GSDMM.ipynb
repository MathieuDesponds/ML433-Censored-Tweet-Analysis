{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6517d207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/robinjaccard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/robinjaccard/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/robinjaccard/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/robinjaccard/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/robinjaccard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Users/robinjaccard/opt/anaconda3/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../helpers_python')\n",
    "from pre_processing import *\n",
    "from lda_helpers import * \n",
    "\n",
    "import gensim\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a195098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'France'\n",
    "\n",
    "# Load data\n",
    "df = load_data_lda(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb0f5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean\"] = df[\"clean\"].apply(lambda x: str(x).split()) # transform the strings into arrays of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1497cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast tweets to numpy array\n",
    "docs = df.clean.to_numpy()\n",
    "\n",
    "# create dictionary of all words in all documents\n",
    "dictionary = gensim.corpora.Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd93afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variable containing size of dictionary\n",
    "vocab_length = len(dictionary)\n",
    "\n",
    "# create bag of words dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab45fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize GSDMM\n",
    "gsdmm = MovieGroupProcess(K=15, alpha=0.1, beta=0.3, n_iters=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d51483c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 742 clusters with 15 clusters populated\n",
      "In stage 1: transferred 493 clusters with 15 clusters populated\n",
      "In stage 2: transferred 403 clusters with 15 clusters populated\n",
      "In stage 3: transferred 305 clusters with 15 clusters populated\n",
      "In stage 4: transferred 286 clusters with 15 clusters populated\n",
      "In stage 5: transferred 263 clusters with 15 clusters populated\n",
      "In stage 6: transferred 283 clusters with 15 clusters populated\n",
      "In stage 7: transferred 248 clusters with 15 clusters populated\n",
      "In stage 8: transferred 283 clusters with 15 clusters populated\n",
      "In stage 9: transferred 236 clusters with 14 clusters populated\n",
      "In stage 10: transferred 233 clusters with 15 clusters populated\n",
      "In stage 11: transferred 246 clusters with 14 clusters populated\n",
      "In stage 12: transferred 232 clusters with 15 clusters populated\n",
      "In stage 13: transferred 233 clusters with 14 clusters populated\n",
      "In stage 14: transferred 237 clusters with 13 clusters populated\n"
     ]
    }
   ],
   "source": [
    "# fit GSDMM model\n",
    "y = gsdmm.fit(docs, vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cc8ad2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic : [ 33  24  77  78  88 102  30  61 298  29   0   0  16  48  16]\n",
      "Most important clusters (by number of docs inside): [ 8  5  4  3  2  7 13  0  6  9  1 14 12 11 10]\n",
      "\n",
      "Cluster 8 : [('white', 155), ('people', 88), ('black', 35), ('say', 32), ('go', 29), ('get', 27), ('u', 26), ('war', 23), ('anti', 23), ('want', 21)]\n",
      "\n",
      "Cluster 5 : [('muslim', 27), ('attack', 15), ('terrorist', 13), ('police', 12), ('year', 12), ('new', 10), ('one', 10), ('islamic', 9), ('amp', 9), ('migrant', 9)]\n",
      "\n",
      "Cluster 4 : [('trump', 22), ('cpac', 12), ('people', 10), ('every', 8), ('want', 8), ('time', 7), ('one', 7), ('president', 7), ('party', 7), ('berry', 6)]\n",
      "\n",
      "Cluster 3 : [('one', 13), ('many', 10), ('people', 9), ('need', 8), ('country', 8), ('biden', 7), ('minister', 7), ('prime', 6), ('first', 6), ('get', 6)]\n",
      "\n",
      "Cluster 2 : [('biden', 15), ('left', 13), ('border', 12), ('wing', 12), ('germany', 11), ('video', 10), ('u', 9), ('open', 8), ('german', 8), ('migrant', 8)]\n",
      "\n",
      "Cluster 7 : [('amp', 18), ('left', 14), ('islamic', 10), ('attack', 9), ('threat', 9), ('like', 8), ('biden', 7), ('terrorist', 7), ('expose', 7), ('control', 6)]\n",
      "\n",
      "Cluster 13 : [('twitter', 11), ('year', 10), ('great', 9), ('follower', 8), ('reset', 7), ('new', 6), ('people', 5), ('think', 5), ('thousand', 4), ('like', 4)]\n",
      "\n",
      "Cluster 0 : [('religion', 5), ('major', 4), ('two', 4), ('boy', 4), ('human', 4), ('anyone', 3), ('jew', 3), ('american', 3), ('religious', 3), ('case', 3)]\n",
      "\n",
      "Cluster 6 : [('woman', 15), ('hijab', 6), ('sharia', 4), ('afrin', 4), ('let', 4), ('cry', 4), ('together', 3), ('amp', 3), ('via', 3), ('day', 3)]\n",
      "\n",
      "Cluster 9 : [('play', 16), ('face', 5), ('scar', 3), ('cpac', 2), ('show', 2), ('rino', 2), ('nikki', 2), ('haley', 2), ('day', 2), ('fantasy', 2)]\n",
      "\n",
      "Cluster 1 : [('nigga', 11), ('soral', 10), ('lil', 10), ('alain', 7), ('answer', 4), ('episode', 3), ('de', 3), ('big', 3), ('piece', 2), ('bravery', 2)]\n",
      "\n",
      "Cluster 14 : [('always', 3), ('marxism', 2), ('shri', 2), ('ji', 2), ('host', 2), ('time', 2), ('social', 2), ('firearm', 2), ('year', 2), ('old', 2)]\n",
      "\n",
      "Cluster 12 : [('mr', 8), ('bit', 3), ('sex', 3), ('enough', 3), ('every', 2), ('people', 2), ('law', 2), ('enforcement', 2), ('earn', 2), ('hate', 2)]\n",
      "\n",
      "Cluster 11 : []\n",
      "\n",
      "Cluster 10 : []\n"
     ]
    }
   ],
   "source": [
    "# print number of documents per topic\n",
    "doc_count = np.array(gsdmm.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-15:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "\n",
    "# define function to get top words per topic\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\nCluster %s : %s\"%(cluster, sort_dicts))\n",
    "\n",
    "# get top words in topics\n",
    "top_words(gsdmm.cluster_word_distribution, top_index, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ea1fc",
   "metadata": {},
   "source": [
    "## Metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70ed68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc90b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get words in topics\n",
    "def get_topics_lists(model, top_clusters, n_words):\n",
    "    '''\n",
    "    Gets lists of words in topics as a list of lists.\n",
    "    \n",
    "    model: gsdmm instance\n",
    "    top_clusters:  numpy array containing indices of top_clusters\n",
    "    n_words: top n number of words to include\n",
    "    \n",
    "    '''\n",
    "    # create empty list to contain topics\n",
    "    topics = []\n",
    "    \n",
    "    # iterate over top n clusters\n",
    "    for cluster in top_clusters:\n",
    "        #create sorted dictionary of word distributions\n",
    "        sorted_dict = sorted(model.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:n_words]\n",
    "         \n",
    "        #create empty list to contain words\n",
    "        topic = []\n",
    "        \n",
    "        #iterate over top n words in topic\n",
    "        for k,v in sorted_dict:\n",
    "            #append words to topic list\n",
    "            topic.append(k)\n",
    "            \n",
    "        #append topics to topics list   \n",
    "        if len(topic) != 0:\n",
    "            topics.append(topic)\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58905e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = get_topics_lists(gsdmm, top_index, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c865a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35696819691336934\n"
     ]
    }
   ],
   "source": [
    "# evaluate model using Topic Coherence score\n",
    "cm_gsdmm = CoherenceModel(topics=topics, \n",
    "                          dictionary=dictionary, \n",
    "                          corpus=bow_corpus, \n",
    "                          texts=docs, \n",
    "                          coherence='c_v')\n",
    "\n",
    "# get coherence value\n",
    "coherence_gsdmm = cm_gsdmm.get_coherence()  \n",
    "\n",
    "print(coherence_gsdmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a1339",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a66cfd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.001, 0.01, 0.1, 0.2]\n",
    "betas = [0.001, 0.01, 0.1, 0.2]\n",
    "n_iters = [5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_coherence = -1000\n",
    "for i in alphas:\n",
    "    for j in betas:\n",
    "        for n in n_iters:\n",
    "            gsdmm = MovieGroupProcess(K=10, alpha=i, beta=j, n_iters=n)\n",
    "            y = gsdmm.fit(docs, vocab_length)\n",
    "            doc_count = np.array(gsdmm.cluster_doc_count)\n",
    "            top_index = doc_count.argsort()[-15:][::-1]\n",
    "            topics = get_topics_lists(gsdmm, top_index, 10) \n",
    "            cm_gsdmm = CoherenceModel(topics=topics, \n",
    "                          dictionary=dictionary, \n",
    "                          corpus=bow_corpus, \n",
    "                          texts=docs, \n",
    "                          coherence='c_v')\n",
    "\n",
    "            # get coherence value\n",
    "            coherence_gsdmm = cm_gsdmm.get_coherence() \n",
    "            if coherence_gsdmm > best_coherence:\n",
    "                best_coherence = coherence_gsdmm\n",
    "            print(\"alpha \"+str(i)+\" betas \"+str(j))\n",
    "            print(\"Coherence: \"+str(coherence_gsdmm))\n",
    "            print(\"Best coherence: \"+str(best_coherence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c569d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model cv coherence\n",
    "alpha = 0.001\n",
    "beta = 0.2\n",
    "iteration = 15\n",
    "coherence_cv = 0.406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fc759d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model u_mass coherence\n",
    "alpha = 0.1\n",
    "beta = 0.01\n",
    "iters = 20\n",
    "coherence_umass = -9.63"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
