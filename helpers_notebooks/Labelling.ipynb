{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f013ee78",
   "metadata": {},
   "source": [
    "# Labelling\n",
    "\n",
    "To assess our results, we labelled 125 french tweets and looked at if the algorithms were able to give the same label as we did. \n",
    "To label the tweets, we first decided on the different labels based on the french government policies, the subject we saw during the exploratory data analysis and the actuality in french society. Then, we used crowdsourcing methods where we three labelled independently the tweets and we then aggregated the results using majority vote.\n",
    "\n",
    "To compare the labelling with the results of the algorithms, we assign to the topics of the algorithms the same labels that we use for the labelling process. Finally, we compare the label of the 125 tweets to the label of the topic it has been assigned by the algorithm and get take the accuracy. (Peut-être que c'est mieux de prendre le F1 score mais peut-être qu'on s'en fout) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95bd62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../helpers_python')\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "653bd5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/to_be_clustered.csv.gz', compression=\"gzip\")\n",
    "country = 'France'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ef4a90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'created_at', 'text', 'source', 'translated', 'clean',\n",
       "       'whcs', 'duplicated', 'language'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e6bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {}\n",
    "categories['France'] = ['white/black - racism', 'islam', 'terrorism', 'coronavirus - vaccination', \n",
    "          'american elections', 'India - Pakistan', 'jew - antisemtism', 'farright', 'anti communism'\n",
    "          'tweeter acccounts', 'fantasy sex play', 'Alain Soral', 'immigration', 'homophobia', 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05627f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_xlsx_for_labelling(country, nb, text_column = 'translated'):\n",
    "    df_c =pd.DataFrame(df[df.whcs == country][text_column])\n",
    "    assert nb < len(df_c)\n",
    "    df_c = df_c.sample(nb)\n",
    "    for cat in categories[country]:\n",
    "        df_c[cat] = 0\n",
    "    df_c.to_excel('data/labelling/'+country+\"_to_be_labeled.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a19bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_xlsx_for_labelling('France', 200, text_column = ['text', 'translated', 'source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf5fd81",
   "metadata": {},
   "source": [
    "## Get back the labeled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d18b5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab = pd.read_excel('../data/labelling/'+country+\"_labeled.xlsx\")\n",
    "categories = ['white/black - racism',\n",
    "       'islam', 'terrorism', 'coronavirus - vaccination', 'american elections',\n",
    "       'India - Pakistan', 'jew - antisemtism', 'farright',\n",
    "       'communism - against bankers , against state  prder ',\n",
    "       'fantasy sex play', 'Alain Soral', 'immigration', 'homophobia',\n",
    "       'tweeter acccounts', 'women', 'other', \"Don't know\"]\n",
    "df_lab[categories] = df_lab[categories].astype(int)\n",
    "df_lab.rename(columns={'Unnamed: 0' : 'index'}, inplace = True)\n",
    "df_lab.set_index('index', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1676a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab['sum'] = df_lab[categories].sum(axis = 1)\n",
    "df_lab = df_lab[df_lab['sum'] > 0]\n",
    "df_lab = df_lab[df_lab[\"Don't know\"] != 1]\n",
    "df_lab['labels'] = df_lab[categories].apply(lambda row: [col for col, b in zip(categories, row) if b],axis=1)\n",
    "df_out = df_lab[['translated','labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2263eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv('data/labelling/'+country+'_final.csv.gz', compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
